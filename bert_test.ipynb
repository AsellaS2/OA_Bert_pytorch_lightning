{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0a27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchsummary import summary\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bert_op import *\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae78551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputpath = 'D:/project/OA_paper/DATA/Part/'\n",
    "savepath = 'D:/project/OA_paper/output/'\n",
    "\n",
    "df = pd.read_csv(inputpath + 'Train.csv', encoding='cp949')\n",
    "\n",
    "x = list(df['rejectionContentDetail'])\n",
    "y = list(map(lambda x: x.split(', '), df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c333e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = x[:1000]\n",
    "y = y[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3aeff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "yt = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347ba290",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 0]\n",
      "[('관념유사', '발음유사', '상품 불명확')]\n",
      "['관념유사' '기타' '발음유사' '상품 불명확' '식별력' '외관유사']\n"
     ]
    }
   ],
   "source": [
    "# Getting a sense of how the tags data looks like\n",
    "print(yt[0])\n",
    "print(mlb.inverse_transform(yt[0].reshape(1,-1)))\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a84eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute no. of words in each content\n",
    "contents = x\n",
    "word_cnt = [len(content.split()) for content in contents]\n",
    "\n",
    "def bigger(x):\n",
    "    return x>512\n",
    "def smaller(x):\n",
    "    return x<513\n",
    "\n",
    "# word_cnt_bigger = list(filter(bigger, word_cnt))\n",
    "word_cnt_smaller = list(filter(smaller, word_cnt))\n",
    "\n",
    "# print(len(word_cnt))\n",
    "# print(len(word_cnt_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.hist(word_cnt_smaller, bins = 40)\n",
    "plt.xlabel('Word Count/Content')\n",
    "plt.ylabel('# of Occurences')\n",
    "plt.title(\"Frequency of Word Counts/sentence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4977d58",
   "metadata": {},
   "source": [
    "### Split the dataset into training ,validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639a7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train in to training and validation\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x, yt, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd29516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 본원상표는 지정상품 &\"칫솔&\"과 관련하여 볼 때 타인의 선등록 400287475 (캐디악)호와 1요부의 칭호, 외관, 관념이 동일유사하므로 상표법 제7조 제1항 제7호에 해당하여 상표등록을 받을 수 없습니다.다만, 당해 지정상품을 삭제 보정하면 다른 거절이유가 없는 한 본원상표는 등록을 받을 수 있습니다.', '   상표법 제10조 제1항 이 출원상표는 아래와 같이 지정상품의 명칭 및 상품류가 불명확하므로 등록을 받을 수 없습니다.  다만, 아래 제시된 상품명을 삭제하는 보정을 하거나 상표법 시행규칙 [별표1]의 상품류 구분표에 예시된 상품명칭을 참고하여 명확한 지정상품명으로 수정하면 그러하지 아니합니다. ○ 지정상품 : 가죽 악세사리 예시) 가죽제 액세서리 - 제26류. 끝.', '  이 출원서비스표 “1++ 한우전문점”은 지정서비스업 “전부”와 관련하여 볼 때,‘최고등급의 한우를 전문으로 판매하는 식당’을 의미하여 서비스업의 성질(품질, 제공내용 등)을 직접적으로 표시한 것으로서 식별력이 없을 뿐만아니라 일반수요자들이 누구의 업무와 관련된 서비스업을 나타내는지 식별할 수 없는 서비스표이므로 상표법 제6조 제1항 제3호 및 제7호에 해당하여 서비스표등록을 받을 수 없습니다. 다만, 지적된 지정서비스업을 삭제하거나 분할하여 출원하는 등의 적법한 조치를 하면 다른 거절이유가 없는 한 이 출원서비스표는 등록받을 수 있습니다. 끝.', ' 본원상표는 &\"환상적인, 멋진등의 뜻으로서 지정상품 중 &\"애완동물용 장난감,크리스마스트리용 받침대,크리스마스트리용 인공눈(雪),크리스마스트리용 촛대,크리스마스트리용벨,합성수지제크리스마스트리,가면,고무제 완구,금속제 완구,깜짝상자,등제 완구,딸랑이,리모트콘트롤을 이용한 작동완구,마스코트인형,모빌,목제(木製) 완구,세트완구,손가락인형,어린이용 걸음마차,어린이용 모형승용차,어린이용 삼륜차,어린이용 흔들목마,완구악기,완구용 공,완구용 블록,완구용 스쿠터,완구용 원반,장난감권총,지제(紙製) 완구,포제(布製)완구,플라스틱제 완구,다트,던지기놀이용 고리,도미노놀이용구,룰렛휠(Roulette wheels),리모트콘트롤을 이용한 작동오락기,마술용구,풍선&\"에 사용하는 경우 그 상품의 품질, 효능을 보통으로 사용하는 방법으로 표시한 상표이므로 상표법 제6조 제1항 제3호에 해당하여 상표등록을 받을 수 없습니다. 끝.', ' 본원상표는 타인의 선등록 400255968(NOVIX)호와 전부의 칭호 유사(관련상품:전부)한 상표이므로 상표법 제7조 제1항 제7호에 해당하여 상표등록을 받을 수 없습니다.', '  상표법 제7조 제1항 제7호 이 출원상표·서비스표는 타인의 선등록상표 제40-723960호(토핑) 및 서비스표 제41-159575호(토핑), 제41-161676호(토핑), 제41-161677호(토핑), 제41-162238호(토핑)의 전부와 호칭이 동일한 상표·서비스표이므로 등록을 받을 수 없습니다. ○ 관련상품 및 서비스업 : 전부   상표법 제3조 이 출원상표·서비스표는 (동일자에 동일한 표장으로 상품 및 서비스업을 포함하여 5개류 출원) 서로 유사한 관계가 없는 다수의 상품을 지정한 경우로서, 상표의 사용의사가 희박하다고 판단되므로 아래와 같이 상표의 사용사실 또는 사용의사를 증명하기 위한 증거서류를 제출하여 주시기 바랍니다. 증명할 서류가 있는 경우에는 의견서에 해당 사항을 첨부하여 제출하시고, 증명할 수 없는 지정상품의 경우에는 해당 지정상품을 삭제하는 보정서를 같이 제출하시면 증거서류에 대한 심사를 통하여 다른 거절이유가 없는 한 등록받을 수 있습니다. (1) 확인대상 지정상품 및 서비스업 증명방법 : 지정상품의 유사군 마다 1개 이상의 지정서비스업의 사용사실 또는 사용의사를 증명하는 서류 제출. (2) 증명할 수 있는 서류(예시) :  ㉠ 사용사실의 증명 : 신문·잡지·카탈로그·전단지 등의 인쇄물, 매장사진, 상품사진, 주문전표·납품서·청구서·영수증 등 거래관련 서류, 국가·공공단체 등이 작성한 서류로서 사용사실을 확인할 수 있는 것, 동업자·거래처·수요자 등의 진술서, 인터넷·신문 등의 관련 기사, 사업자등록증, 상호등기부등본 등 ㉡ 사용의사의 증명 : 출원 후 3~4년 내에 상표의 사용을 개시할 의사를 표시한 상표 사용계획서, 지정상품의 구체적 내용과 상표의 개시시기를 포함하는 출원인의 진술서, 상품의 기획, 공장이나 점포의 건설, 임대 등 사업의 준비상황이나 계획에 관한 자료 등. 끝.', '  이 출원상표 “가방 나르는 가방”는 지정상품의 보통명칭으로서 물건을 담아 몸에 지니고 다니거나 운반하는 기능을 가진 말인‘가방’과,‘물건을 한 곳에서 다른 곳으로 옮기다’는 의미를 지닌‘나르다’로 구성된 것으로서 이를 지정상품(가방류)에 사용하는 경우 ‘가방을 나른다거나, 가방을 담아 나른다’는 등의 의미로 인식되어 현실적으로 수요자가 누구의 업무와 관련된 상품을 표시하는 상표인지를 식별할 수 없으므로 상표법 제6조 제1항 제7호에 해당하여 상표등록을 받을 수 없습니다.', '   구 상표법(2016.2.29. 법률 제14033호에 의하여 개정되기 전의 것) 제7조 제1항 제7호 이 출원상표는 아래와 같이 타인의 선등록상표와 표장 및 지정상품이 동일 또는 유사한 상표이므로 등록을 받을 수 없습니다. 다만, 선등록상표의 지정상품과 동일 또는 유사한 아래 지정상품을 삭제하는 보정을 하거나 구 상표법(2016.2.29. 법률 제14033호에 의하여 개정되기 전의 것) 제18조의 규정에 따라 분할하여 출원하는 경우에는 그러하지 아니합니다.○ 국제등록번호 제1304998호, “FARADAY”와 본원상표의 1요부인 “FARADAY”의 칭호 및 관념이 동일, 유사합니다.○ 지정상품 - G3823 육상 차량용 엔진  - G390101 육상 차량용 모터. 끝.', '   상표법 제8조 제1항 이 출원서비스표는 아래와 같이 타인의 선출원서비스표와 표장 및 지정서비스업이 동일 또는 유사한 서비스표이므로 등록을 받을 수 없습니다. 다만, 선출원서비스표의 지정서비스업과 동일 또는 유사한 아래 지정서비스업을 삭제하는 보정을 하거나 상표법 제18조의 규정에 따라 분할하여 출원하는 경우에는 그러하지 아니합니다. 또한, 타인의 선출원서비스표가 거절결정, 출원취하, 출원포기 또는 무효가 확정된 경우에도 그러하지 아니합니다.○ 타인의 선출원 제41-2012-0006059호(폴프랑 PORTFRANC)○ 지정서비스업 - 관광숙박업, 리조트숙박업, 모텔업, 유스호스텔업, 임시숙박시설임대업, 캠프숙박시설 예약업, 콘도미니엄업, 팬션업, 호스텔업, 호텔업, 회원제 숙박설비 운영업, 휴일캠프숙박서비스업 끝.', ' 본원상표는 그 지정상품 중 공업용 비누, 비의료용 방향제, 약용 크림, 약용 화장수, 약용 비누‘은 불명확하므로 상표법 제10조에 해당하여 상표등록을 받을 수 없습니다. 다만, 지적된 상품명을 삭제하거나 상표법 시행규칙 [별표1]의 상품류구분표에 예시된 상품명칭을 참고하여 명확한 지정상품명으로 수정하면 다른 거절이유가 없는 한 본원상표는 등록받을 수 있습니다.※ 명확한 상품명(예시) :* 공업용 금속비누/1류 G1001* 가정용 방향제 3류 G1202, 인체용 방향제 3류 G1201B* 비의료용 약용 크림, 비의료용 약용 화장수 3류 G1201B* 비의료용 약용비누 3류 G1301B.※ 지정상품은 용도, 재료 등에 따라 상품류 등이 달라지므로 지적상품을 “용도+재료+방식+상품명칭” 형태로 구체적으로 기술 필요. 끝.']\n"
     ]
    }
   ],
   "source": [
    "print(x_tr[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac8053",
   "metadata": {},
   "source": [
    "### Preparing the Dataset and DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422db7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bert tokenizer\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "Bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "826b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters that will be use for training\n",
    "N_EPOCHS = 1\n",
    "BATCH_SIZE = 8\n",
    "MAX_LEN = 300\n",
    "LR = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d4a7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and set up the data_module\n",
    "OAdata_module = TrainOADataModule(x_tr=x_tr, y_tr=y_tr, x_val=x_val, y_val=y_val, tokenizer=Bert_tokenizer,\n",
    "                                  batch_size=BATCH_SIZE, max_token_len=MAX_LEN)\n",
    "OAdata_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b40cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier model\n",
    "steps_per_epoch = len(x_tr)//BATCH_SIZE\n",
    "model = OAClassifier(n_classes=6, steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae3f2ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'attn_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-27b117b2eabd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\project\\oa_paper\\venv\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# make a forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# remove these hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\oa_paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'attn_mask'"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ea61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Pytorch Lightning callback for Model checkpointing\n",
    "\n",
    "# saves a file like: input/OA-epoch=02-val_loss=0.32.ckpt\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',                         # monitored quantity\n",
    "    filename='OA-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,                               # save the top 3 models\n",
    "    mode='min',                                 # mode of the monitored quantity  for optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671eeadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Model Trainer\n",
    "trainer = pl.Trainer(max_epochs=N_EPOCHS, gpus=1, callbacks=[checkpoint_callback], progress_bar_refresh_rate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca936b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb64782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert       | BertModel         | 177 M \n",
      "1 | classifier | Linear            | 4.6 K \n",
      "2 | criterion  | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.432   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cbb88d46a946adab736f5986919549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\oa_paper\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "d:\\project\\oa_paper\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccad23116c8461287d976bed5108176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the Classifier Model\n",
    "trainer.fit(model, OAdata_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d663a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_loss': 0.7495506405830383}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.7495506405830383}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model performance on the validation dataset\n",
    "trainer.validate(model,datamodule=OAdata_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26af42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the checkpoint path for best model\n",
    "model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d541527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all contents in x_test\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for content in x_val:\n",
    "    encoded_con = Bert_tokenizer.encode_plus(\n",
    "        content,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Add the input_ids from encoded content to the list.\n",
    "    input_ids.append(encoded_con['input_ids'])\n",
    "    # Add its attention mask\n",
    "    attention_masks.append(encoded_con['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1da8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3272ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader.\n",
    "pred_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "pred_sampler = SequentialSampler(pred_data)\n",
    "pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2207ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_pred_outs = 0\n",
    "flat_true_labels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4eb951c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OAClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model in evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7699a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking variables\n",
    "pred_outs, true_labels = [], []\n",
    "\n",
    "# Predict\n",
    "for batch in pred_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device).long() for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_attn_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        pred_out = model(b_input_ids, b_attn_mask)\n",
    "        pred_out = torch.sigmoid(pred_out)\n",
    "        # Move predicted output and labels to CPU\n",
    "        pred_out = pred_out.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    # Store predictions and true labels\n",
    "    pred_outs.append(pred_out)\n",
    "    true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3c34448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches.\n",
    "flat_pred_outs = np.concatenate(pred_outs, axis=0)\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f0cc9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define candidate threshold values\n",
    "threshold = np.arange(0.4, 0.51, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e21892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert probabilities into 0 or 1 based on a threshold value\n",
    "def classify(pred_prob, thresh):\n",
    "    y_pred = []\n",
    "    for tag_label_row in pred_prob:\n",
    "        temp = []\n",
    "        for tag_label in tag_label_row:\n",
    "            if tag_label >= thresh:\n",
    "                temp.append(1)  # Infer tag value as 1 (present)\n",
    "            else:\n",
    "                temp.append(0)  # Infer tag value as 0 (absent)\n",
    "        y_pred.append(temp)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fc237f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []  # Store the list of f1 scores for prediction on each threshold\n",
    "\n",
    "# convert labels to 1D array\n",
    "y_true = flat_true_labels.ravel()\n",
    "y_score = flat_pred_outs.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71014658",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in threshold:\n",
    "    # classes for each threshold\n",
    "    pred_bin_label = classify(flat_pred_outs, thresh)\n",
    "\n",
    "    # convert to 1D array\n",
    "    y_pred = np.array(pred_bin_label).ravel()\n",
    "\n",
    "    scores.append(metrics.f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7825bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold Value = 0.4\n"
     ]
    }
   ],
   "source": [
    "# find and save the optimal threshold\n",
    "opt_thresh = threshold[scores.index(max(scores))]\n",
    "# f = open(savepath + \"opt_thresh.txt\", 'w')\n",
    "# f.write(str(opt_thresh))\n",
    "# f.close()\n",
    "print(f'Optimal Threshold Value = {opt_thresh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27da282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for optimal threshold\n",
    "y_pred_labels = classify(flat_pred_outs, opt_thresh)\n",
    "y_pred = np.array(y_pred_labels).ravel()  # Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afddebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score  support\n",
      "0              0.800000  0.013937  0.027397   861.00\n",
      "1              0.283544  0.991150  0.440945   339.00\n",
      "accuracy       0.290000  0.290000  0.290000     0.29\n",
      "macro avg      0.541772  0.502544  0.234171  1200.00\n",
      "weighted avg   0.654101  0.290000  0.144224  1200.00\n"
     ]
    }
   ],
   "source": [
    "# report 작성\n",
    "report = metrics.classification_report(y_true=y_true, y_pred=y_pred, output_dict=True)\n",
    "report = pd.DataFrame(report).transpose()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7087d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlb.inverse_transform(np.array(y_pred_labels))\n",
    "y_act = mlb.inverse_transform(flat_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장용 output\n",
    "# output = pd.DataFrame({'Body':x_val,'Actual labels':y_act,'Predicted labels':y_pred})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
